{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is sequence tagging? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sequence Tagging aka. Sequence Labeling\n",
    "\n",
    "It's an _annotation task_\n",
    "\n",
    "\"sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values.\" [1]\n",
    "\n",
    "\n",
    "### Prototypical annotation task:\n",
    "\"[...] annotators assign labels to specific items (words, segments etc.) in the source.\" [2]\n",
    "\n",
    "[1] https://en.wikipedia.org/wiki/Sequence_labeling <br/>\n",
    "[2] Artstein, 2017\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does crowd sourcing work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why use crowd sourcing for sequence tagging? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problem with deep learning approaches:\n",
    "- data intensive: ten thousands (and more) labelled documents for training\n",
    "- new tasks/new domains: labelling is time consuming/costly (pay experts)\n",
    "\n",
    "### Solution:\n",
    "- crowdsourcing: large number of untrained workers annotate documents\n",
    "\n",
    "### New problem:\n",
    "- multiple sequences of unreliable labels per document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What's the problem? Reliability \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is reliability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Goal\n",
    "\n",
    "- consistent and reproducible annotation process (which means \"reliable\")\n",
    "\n",
    "### How to assess the annotation process?\n",
    "\n",
    "- measure inter-annotator agreement on same source data\n",
    "- answers the questions how consistent/reproducible an annotation process is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Causality\n",
    "Rationale for measuring agreement [1]\n",
    "- agreement among annotators\n",
    "\n",
    "demonstrates\n",
    "\n",
    "- Reliable annotation process\n",
    "\n",
    "necessary but not sufficient for\n",
    "\n",
    "- Correct annotations\n",
    "\n",
    "[1] Artstein, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Iterative Annotation Process\n",
    "- use sample of the data\n",
    "- based on written guidelines for annotators\n",
    "- test reliability of annotations produced by the guidelines\n",
    "- adapt guidelines\n",
    "- iterate!\n",
    "- if reliable: proceed to annotate the rest of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which problems occur in an annotation process? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Measure inter-annotator agreement\n",
    "\n",
    "- compute single coefficient\n",
    "\n",
    "- raw/observed agreement: \n",
    "  - for 2 annotators, count number of identical labels\n",
    "  - divide by number of all items to be annotated\n",
    " \n",
    "\n",
    "- Problem:\n",
    "  - accidental agreement\n",
    "  - Example: gene sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to measure them? Overall and the nuances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Measure inter-annotator agreement\n",
    "- better: measure amount of agreement above chance\n",
    "- $A_o$: amount of observed inter-annotator agreement\n",
    "- $A_e$: amount of inter-annotator agreement expected by random coding model\n",
    "- $A_o - A_e$: amount of agreement above chance\n",
    "- $1 - A_e$: maximal agreement\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\kappa, \\pi, ... = \\frac{A_o - A_e}{1 - A_e}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Measure inter-annotator agreement\n",
    "\n",
    "Other approach: measure disagreement\n",
    "\n",
    "- $D_o = 1 - A_o$\n",
    "- $D_e = 1 - A_e$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\alpha = 1 - \\frac{D_o}{D_e}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Advantage: allows to express disagreement in other units than percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Concrete Coefficients: observed (dis)agreement\n",
    "- Fleiss's $\\kappa$\n",
    "- Krippendorff's $\\alpha$\n",
    "\n",
    "They differ the way they compute observed agreement/expected random agreement.\n",
    "\n",
    "- Fleiss's $\\kappa$:\n",
    "  - observed agreement\n",
    "  - similar to raw agreement\n",
    "  - all disagreement are treated equally\n",
    "- Krippendorff's $\\alpha$:\n",
    "  - observed disagreement\n",
    "  - distance function per pair of labels\n",
    "  - scales the disagreement \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Concrete Coefficients: expected agreement\n",
    "- Fleiss's $\\kappa$\n",
    "- Krippendorff's $\\alpha$\n",
    "\n",
    "They differ the way they compute observed agreement/expected random agreement.\n",
    "\n",
    "- Fleiss's $\\kappa$:\n",
    "  - similar to raw agreement\n",
    "  - all disagreements are treated equally\n",
    "- Krippendorff's $\\alpha$:\n",
    "  - distance function per pair of labels\n",
    "  - scales the disagreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problem of single coefficient\n",
    "\n",
    "- cannot capture complex aspects of the annotation process\n",
    "- fine grained anaylsis necessary\n",
    "\n",
    "Four factors of complications:\n",
    "- diversity in underlying data\n",
    "- similarities between labels\n",
    "- differences in the difficulty of individual items\n",
    "- differences between individual annotators and annotator populations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Diversity in the underlying data\n",
    "\"When studying annotation of heterogeneous data, \n",
    "agreement should be calculated and reported for \n",
    "the homogeneous subparts of the data, in \n",
    "addition to the data as a whole.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Similarities between the labels\n",
    "“When annotation labels have an internal structure, \n",
    "it may be acceptable to calculate agreement on \n",
    "different aspects of the same annotation. This is \n",
    "justified when the different aspects reflect separate \n",
    "and distinct decisions made by the annotators, thus \n",
    "reflecting different facets of a complex annotation process.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Differences in the difficulty of individual items\n",
    "\"To identify the extent of individual item difficulty, it is recommended to conduct a reliability study with multiple annotators.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Differences between annotators/annot. populations\n",
    "\n",
    "\"In a reliability study with more than two annotators, \n",
    "differences between the annotators should be investigated \n",
    "by calculating agreement among subgroups of annotators.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does a Bayesian approach help? Model uncertainty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How is sequential data modelled?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How are Bayes and the sequential model combined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
